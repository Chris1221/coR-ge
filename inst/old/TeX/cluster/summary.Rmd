---
title: "FDR by Cluster"
author: "Christopher Cole"
date: "January 12, 2016"
output: 
  html_document:
    highlight: pygments
---

### 1. LD Scoring

Clone the repository with modifications and ensure dependencies are installed:
  
```{r, engine = 'bash', eval = FALSE}
git clone https://github.com/Chris1221/ldsc.git
```

Check for errors:

```{r, engine = 'bash', eval = FALSE}
brew doctor
cd ldsc
./ldsc.py -h
./munge_sumstats.py -h
```

Download chromosome 1 data in plink format and run ld score regression to obtain ld scores. Note that the anaconda version of python must be used in place of default python `2.7.x` in `/usr/bin/env` to properly use all packages. 


**NOTE THAT WE ARE USING THE WRONG VERSION OF 1000G SO EVERYTHING BELOW IS SUBJECT TO VALIDATION**

```{r, engine = 'bash', eval = FALSE}
./ldsc.py --bfile chr1 --l2 --ld-wind-cm 1 --keep-maf --out chr1
```

Giving chr1.ldscore.gz

```{bash, eval = FALSE}
gunzip chr1.ldscore.gz
```


```{r, eval = FALSE}
library(data.table)
setwd("ldsc")
chr1 <- fread("chr1.ldscore", h = T)
head(chr1)


        CHR        SNP        BP          CM         MAF        L2
     1:   1 rs12565286    721290   0.4102920 0.043535620 47.085347
     2:   1  rs3094315    752566   0.4887759 0.163588391 71.322571
     3:   1  rs3131972    752721   0.4888678 0.167546174 70.189802
     4:   1  rs3131969    754182   0.4897337 0.133245383 78.250540
     5:   1  rs1048488    760912   0.4925073 0.203166227 67.550755
    ---                                                           
108284:   1  rs4926502 249209140 293.3892100 0.019788918  3.850692
108285:   1  rs6704311 249210707 293.3905500 0.063324538 74.245025
108286:   1 rs34013644 249211350 293.3910900 0.048812665 68.212386
108287:   1 rs12746903 249218992 293.3970000 0.007915567 41.130897
108288:   1 rs12726733 249222473 293.3970000 0.026385224  5.025715

```

## 2. Clustering

Import necessary functions ([Source](http://www.r-statistics.com/2013/08/k-means-clustering-from-r-in-action/))

```{r, eval = F}
wssplot <- function(data, nc=15, seed=1234){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Number of Clusters",
                     ylab="Within groups sum of squares")}
```

1. Determine number of clusters by looking at descent of $SS_{wg}$

```{r, eval = F}
wssplot(as.data.frame(cbind(chr1$MAF, chr1$L2))) 
```
![](/Users/Chris/repos/coR-ge/assets/images/ss_wg.png)

Noticing a marked decrease to 4 centroids, we hypothesize that $k=4$. To further validate, we attempt to validate in `NbClust`.

```{r, eval = F}
nc <- NbClust(as.data.frame(cbind(chr1$MAF, chr1$L2)), min.nc=2, max.nc=15, method="kmeans")
```

Note that this is currently too big to fit into memory, so will have to be qued. For now we will stick to $n=4$ cluster. 

Now we do the actual clustering

```{r, eval = F}
k <- kmeans(as.data.frame(cbind(chr1$MAF, chr1$L2)), 4, nstart = 25)
plot(chr1$MAF, chr1$L2, col = k$cluster)
```

![](/Users/Chris/repos/coR-ge/assets/images/k.png)

This isn't exactly what we were expecting. Trying again but this time scaling the variables:

```{r, eval = F}

scaled_chr1 <- scale(as.data.frame(cbind(chr1$MAF, chr1$L2)))
sk <- kmeans(scaled_chr1, 4, nstart = 25)
plot(chr1$MAF, chr1$L2, col = sk$cluster)

```

![](/Users/Chris/repos/coR-ge/assets/images/sk.png)

These look more reasonable. We go forward with $n=4$ though $n=5$ also gives interesting results.

![](/Users/Chris/repos/coR-ge/assets/images/sk5.png)

We write the clusters to the data frame and continue to identifying FDR ranges:

```{r, eval = F}
chr1$cluster <- sk$cluster
head(chr1)

            CHR        SNP        BP          CM         MAF        L2 cluster
         1:   1 rs12565286    721290   0.4102920 0.043535620 47.085347       4
         2:   1  rs3094315    752566   0.4887759 0.163588391 71.322571       4
         3:   1  rs3131972    752721   0.4888678 0.167546174 70.189802       4
         4:   1  rs3131969    754182   0.4897337 0.133245383 78.250540       4
         5:   1  rs1048488    760912   0.4925073 0.203166227 67.550755       4
        ---                                                                   
    108284:   1  rs4926502 249209140 293.3892100 0.019788918  3.850692       4
    108285:   1  rs6704311 249210707 293.3905500 0.063324538 74.245025       4
    108286:   1 rs34013644 249211350 293.3910900 0.048812665 68.212386       4
    108287:   1 rs12746903 249218992 293.3970000 0.007915567 41.130897       4
    108288:   1 rs12726733 249222473 293.3970000 0.026385224  5.025715       4

write.table(chr1, "cluster_l2.txt", col.names =T, row.names = F, quote = F)
```

Transfer to cluster:

```{r, engine = 'bash', eval = FALSE}
scp cluster_l2.txt hpc2862@130.15.59.64:/scratch/hpc2862/CAMH/perm_container/out_DONTDELETE_files/cluster_l2.txt
```

Note: set up rsa keys to automate this

### 3. FDR

After transfering the cluster file to the cluster, we combine it with the snp information file `snp_summary.out`. We then subset to only snps which have available $k$.

```{r, eval = F}
summary <- fread("/scratch/hpc2862/CAMH/perm_container/snp_summary.out", h = T, sep = " ")
cluster <- fread("/scratch/hpc2862/CAMH/perm_container/out_DONTDELETE_files/cluster_l2.txt", h = T)


merge(summary, cluster, by = "rsid", all.x = T, all.y = F) -> summary2
summary2 %>% filter(!is.na(k)) -> summary3

```

**Note:**

> Because of the low overlap we have an overrepresentation of cluster 1 and 4 and under representation of 2 and 3. This will correct itself when we use the correction version of 1000G and/or transition to UK10K. 

```{r, eval = F}
> sum(summary3$k == 1)
[1] 39894
> sum(summary3$k == 2)
[1] 4665
> sum(summary3$k == 3)
[1] 17741
> sum(summary3$k == 4)
[1] 41475
```

Writing the file back out, we can tailor the analysis to this

```{r, eval = F}

write.table(summary3, "/scratch/hpc2862/CAMH/perm_container/snp_summary2.out", quote = F, row.names = F, col.names = T)
```

Altering the following files:

```{r, engine = 'bash', eval = FALSE}
clean_comb.sh
correct_and_report.sh
rand.R
```

We do the following in the analysis stage:

1. Read in `*.gen*` 
2. Subset all possibly SNPs to be chosen to be only those with cluster information
3. Select $n_{real}=1000$ SNPs from each of the strata ($k=4$)
4. Assign effect sizes a la DEPICT
5. Write out
6. Plink analyze to `plink.qassoc`
7. Read everything in again
8. Randomly assign half the effect SNPs to stratum 1 $S_1$ and half to $S_2$
8. Exclude already selected effect SNPs and randomly select $n_{fake}=3000$
9. Correct and report aggregated and stratefied FDRs 

Note that at the moment, the addition of the cluster is causing some problems in the scoring routine. Nothing too large.

## Topics for discussion

**Things which are done**

1. Imputation (somatic)
    + Just sending to Meghan
2. LD Score vs. MAF clustering
    + Almost, few small errors
3. Polygenic risk score for SCZ paper
    + [here](http://www.schres-journal.com/article/S0920-9964(15)30087-6/abstract?rss=yes)

**Things which are not done**

1. True positive gradient by $R^2$
2. UK10K Application
    + See email convo with Natalie
3. mineR paper
    + Journal of Open Research Software
    + Bioinformatics
    + Journal of Statistical Software
  
  
**Things to do next**

1. Co-op Position 
2. Run entire analysis on UK10K Data when it becomes available
    + After clustering is working, finish up mineR and hopefully submit
3. When UK10K is available, rerun everything on it then decide what conclusions to make, decide where to go with coR-ge after that

```{r, eval = TRUE}

library(rgl)
example(rgl)

```



